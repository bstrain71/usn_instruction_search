{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "usn_instruction_search.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bstrain71/usn_instruction_search/blob/master/usn_instruction_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_LG-8eNj7CV",
        "colab_type": "text"
      },
      "source": [
        "### INGEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqstC-ucG4bd",
        "colab_type": "text"
      },
      "source": [
        "OPNAV Instructions: https://www.secnav.navy.mil/doni/opnav.aspx?RootFolder=%2Fdoni%2FDirectives%2F01000%20Military%20Personnel%20Support&FolderCTID=0x012000E8AF0DD9490E0547A7DE7CF736393D04&View=%7BCACF3AEF%2DAED4%2D433A%2D8CE5%2DA45245715B5C%7D\n",
        "\n",
        "Note: This example only contains series 01-01 through 01-500 as it is just for demonstration. Here is a key to their topics:\n",
        "\n",
        " \n",
        "01-01 General Military Personnel Records\n",
        "\n",
        "01-100 General Recruiting Records \n",
        "\n",
        "01-200 Personnel Classification and Designation \n",
        "\n",
        "01-300 Assignment and Distribution Services  \n",
        "\n",
        "01-400 Promotion and Advancement Programs \n",
        "\n",
        "01-500 Military Training and Education Services \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOt-HPeeeNV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoXJ2TGoj93q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78fb0ca0-1537-4bf1-db3f-e1897a66e55a"
      },
      "source": [
        "# Load gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu_BNqILquGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tika #install tika"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWcmqjWNpN6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tika import parser\n",
        "\n",
        "# Test code for seeing how tika works.\n",
        "#raw = parser.from_file('/content/gdrive/My Drive/MSDS_422/OPNAV_Instructions/1000.16L With CH-2.pdf')\n",
        "# raw['content'] gives the raw text of the pdf\n",
        "#print(raw['content'])\n",
        "#raw.keys()\n",
        "#raw['metadata']['resourceName'] # Calls the file name - not metadata. not every file has metadata so don't rely on it."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD2Q8rZGvnV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# The indices for these are the same so they can be zipped later.\n",
        "# The .pdf filename is what is read, not the metadata, so whatever\n",
        "# the file is called in the folder is what it will be listed as\n",
        "# in the results.\n",
        "\n",
        "documents = [] # Contains raw text from all .pdfs.\n",
        "filenames = [] # Contains the filenames of the .pdfs\n",
        "\n",
        "# Parse all .pdf contents and their filenames.\n",
        "directory = '/content/gdrive/My Drive/MSDS_422/OPNAV_Instructions/'\n",
        "for file in os.listdir(directory):\n",
        "  temp = parser.from_file(directory+file)\n",
        "  documents.append(temp['content'])\n",
        "  filenames.append(temp['metadata']['resourceName'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66RdSdPi9lvl",
        "colab_type": "text"
      },
      "source": [
        "### EDA and Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-VIDz859ODh",
        "colab_type": "text"
      },
      "source": [
        "Make everything in the document lowercase and remove stopwords. The stopwords list can be modified as needed, this one comes directly from the gensim documentation and is relatively standard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STJ0daY4eNWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora\n",
        "\n",
        "# improved list from Stone, Denis, Kwantes (2010)\n",
        "STOPWORDS = \"\"\"\n",
        "a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be\n",
        "became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can\n",
        "cannot cant co computer con could couldnt cry de describe\n",
        "detail did didn do does doesn doing don done down due during\n",
        "each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen\n",
        "fify fill find fire first five for former formerly forty found four from front full further get give go\n",
        "had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie\n",
        "if in inc indeed interest into is it its itself keep last latter latterly least less ltd\n",
        "just\n",
        "kg km\n",
        "made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely\n",
        "neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off\n",
        "often on once one only onto or other others otherwise our ours ourselves out over own part per\n",
        "perhaps please put rather re\n",
        "quite\n",
        "rather really regarding\n",
        "same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten\n",
        "than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under\n",
        "until up unless upon us used using\n",
        "various very very via\n",
        "was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you\n",
        "your yours yourself yourselves\n",
        "\"\"\"\n",
        "\n",
        "# remove common words and tokenize\n",
        "stoplist = set(STOPWORDS.split())\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "# Building the corpus.\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOQTYvds9tn6",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ReXI7pk83bp",
        "colab_type": "text"
      },
      "source": [
        "Build the model to measure similarity between documents. Here we will use Latent Semantic Analysis (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klcmgU3PeNWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import models\n",
        "# LSI uses singular value decomposition - I think that\n",
        "# num_topics corresponds do the number of singular values\n",
        "# to use.\n",
        "# \"Indexing by Latent Semantic Analysis\" <http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf>\n",
        "# Latent Semantic Indexing <https://en.wikipedia.org/wiki/Latent_semantic_indexing>\n",
        "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpXBeol5_GY4",
        "colab_type": "text"
      },
      "source": [
        "Now suppose a user typed in the query [whatever doc = 'user stuff']. We would like to sort the corpus documents in decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of possible similaritiesâ€”on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks, just a semantic extension over the boolean keyword match:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If35NwYmeNWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "47162735-fa43-49ce-ea61-19d90fa2b5e9"
      },
      "source": [
        "# Here the 'doc' object is the user's search term.\n",
        "# vec_bow cleans the user's search term\n",
        "# vec_lsi is a list of tuples which are the similarities,\n",
        "\n",
        "# This is only the boolean similarity!\n",
        "# This is the same as doing Ctrl+F and counting\n",
        "# the number of hits.\n",
        "\n",
        "doc = \"comissioning programs\"\n",
        "\n",
        "\n",
        "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
        "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
        "print(vec_lsi)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, -0.0038342718449363325), (1, 0.07585440349438378), (2, -0.01619275848512994), (3, -0.04147139329615), (4, 0.031191086499459866), (5, -0.014146373319021069), (6, -0.015675283527503595), (7, -0.015469876650197887), (8, 0.017868974994854056), (9, -0.06476817847306676), (10, -2.7406325151583994e-05), (11, 0.0034171494353621478), (12, 0.040851701515736805), (13, -0.002103609456464999), (14, -0.09619194804537322), (15, 0.05064201487262554), (16, 0.013998997332887415), (17, -0.0071289636718904), (18, -0.048056618011155405), (19, -0.04148487831031291)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDc4LmyneNWp",
        "colab_type": "text"
      },
      "source": [
        "In addition, we will be considering `cosine similarity <http://en.wikipedia.org/wiki/Cosine_similarity>`_\n",
        "to determine the similarity of two vectors. Cosine similarity is a standard measure\n",
        "in Vector Space Modeling, but wherever the vectors represent probability distributions,\n",
        "`different similarity measures <http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence>`_\n",
        "may be more appropriate.\n",
        "\n",
        "Initializing query structures\n",
        "++++++++++++++++++++++++++++++++\n",
        "\n",
        "To prepare for similarity queries, we need to enter all documents which we want\n",
        "to compare against subsequent queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxRponkfeNWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e19ec576-a630-46d5-a037-55a925316c8e"
      },
      "source": [
        "from gensim import similarities\n",
        "index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. [matutils.py:737]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQtEyMiCeNWx",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The class :class:`similarities.MatrixSimilarity` is only appropriate when the whole\n",
        "  set of vectors fits into memory. For example, a corpus of one million documents\n",
        "  would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.\n",
        "\n",
        "  Without 2GB of free RAM, you would need to use the :class:`similarities.Similarity` class.\n",
        "  This class operates in fixed memory, by splitting the index across multiple files on disk, called shards.\n",
        "  It uses :class:`similarities.MatrixSimilarity` and :class:`similarities.SparseMatrixSimilarity` internally,\n",
        "  so it is still fast, although slightly more complex.</p></div>\n",
        "\n",
        "Index persistency is handled via the standard :func:`save` and :func:`load` functions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nApicIzleNWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7f0fb732-b1ba-4fe7-da71-5f10f59b7809"
      },
      "source": [
        "index.save('/tmp/deerwester.index')\n",
        "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function [smart_open_lib.py:402]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZFOVo4veNW4",
        "colab_type": "text"
      },
      "source": [
        "This is true for all similarity indexing classes (:class:`similarities.Similarity`,\n",
        ":class:`similarities.MatrixSimilarity` and :class:`similarities.SparseMatrixSimilarity`).\n",
        "Also in the following, `index` can be an object of any of these. When in doubt,\n",
        "use :class:`similarities.Similarity`, as it is the most scalable version, and it also\n",
        "supports adding more documents to the index later.\n",
        "\n",
        "Performing queries\n",
        "++++++++++++++++++\n",
        "\n",
        "To obtain similarities of our query document against the nine indexed documents:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT5kGVeSeNW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6e91fabd-3543-4f7a-cdc6-8aca6e22da66"
      },
      "source": [
        "# index is the similarity index, or what we are checking against (corpus in the lsi space)\n",
        "# vec_lsi is our query in the lsi space\n",
        "# the output is the similarity tuples\n",
        "\n",
        "sims = index[vec_lsi]\n",
        "\n",
        "similarity_list = list(zip(sims, filenames))\n",
        "#print(similarity_list)\n",
        "print(sorted(similarity_list, reverse = True))"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.6071481, '1520.23C w CH-2.pdf'), (0.5918833, '1520.37B.pdf'), (0.584865, '1520.24D.pdf'), (0.572532, '1500.72G.pdf'), (0.5686546, '1520.31D.pdf'), (0.54997236, '1560.9A.pdf'), (0.5350164, '1520.41A.pdf'), (0.52676916, '1524.2.pdf'), (0.5145358, '1500.78.pdf'), (0.49105394, '1754.1B.pdf'), (0.48965767, '1520.18J.pdf'), (0.4611439, '1531.6D.pdf'), (0.4563515, '1110.1B.pdf'), (0.4537352, '1520.38A.pdf'), (0.4446472, '1500.83A.pdf'), (0.4399071, '1520.42A CH-1.pdf'), (0.43936783, '1420.1B OPNAV.pdf'), (0.43754464, '1520.44.pdf'), (0.4008643, '1500.85.pdf'), (0.40060136, '1710.9.pdf'), (0.39065617, '1520.40B.pdf'), (0.38449755, '1542.4E.pdf'), (0.38084587, '1738.1A.pdf'), (0.3797332, '1550.12A.pdf'), (0.3788957, '1500.64C.pdf'), (0.37437063, '1120.13B.pdf'), (0.37177193, '1650.30A.pdf'), (0.37131748, '1700.9E CH-1.pdf'), (0.36844933, '1650.30A CH-1.pdf'), (0.35999912, '1754.2F.pdf'), (0.35883355, '1540.56B.pdf'), (0.35875905, '1306.3C.pdf'), (0.3555308, '1740.6.pdf'), (0.35461897, '1700.7E.pdf'), (0.35315573, '1210.5B.pdf'), (0.3483267, '1531.5E.pdf'), (0.34667262, '1301.10C.pdf'), (0.34613234, '1742.1C.pdf'), (0.34212202, '1740.4E.pdf'), (0.33852258, '1500.77A.pdf'), (0.3379268, '1542.5C.pdf'), (0.33693466, '1414.9B.pdf'), (0.33354226, '1770.3A.pdf'), (0.32484028, '1770.1B.pdf'), (0.32427996, '1770.2B.pdf'), (0.3227175, '1301.11.pdf'), (0.31481266, '1780.4.pdf'), (0.314689, '1560.10D.pdf'), (0.31280664, '1530.4035.pdf'), (0.31018692, '1412.11.pdf'), (0.30818576, '1754.8.pdf'), (0.3041653, '1520.43B.pdf'), (0.30257717, '1754.7A OPNAV.pdf'), (0.30189118, '1530.4033.pdf'), (0.300864, '1700.13C CH-1.pdf'), (0.2985096, '1540.51E.pdf'), (0.29843536, '1650.35A.pdf'), (0.29675654, '1700.16B.pdf'), (0.29529765, '1720.3G.pdf'), (0.29458854, '1000.1N.pdf'), (0.2926559, '1500.27G.pdf'), (0.29239464, '1120.3B.pdf'), (0.28953266, '1520.39A.pdf'), (0.2876191, '1542.7E.pdf'), (0.2873647, '1750.1G W CH-2.pdf'), (0.27696455, '1551.11B.pdf'), (0.2747008, '1650.33A.pdf'), (0.27342665, '1220.2B.pdf'), (0.27330098, '1430.4C.pdf'), (0.27195585, '1571.1B.pdf'), (0.26891038, '1530.8A.pdf'), (0.2678243, '1306.2H w CH-1.pdf'), (0.26707917, '1700.17.pdf'), (0.26584494, '1300.17B.pdf'), (0.26305163, '1520.36C.pdf'), (0.26166153, '1412.15.pdf'), (0.26142198, '1412.14.pdf'), (0.2611148, '1210.2C.pdf'), (0.25674984, '1300.14D.pdf'), (0.2534161, '1900.2C.pdf'), (0.2529941, '1306.4A.pdf'), (0.2521652, '1510.10D.pdf'), (0.25086302, '1710.11.pdf'), (0.24467754, '1534.1E.pdf'), (0.24414791, '1740.3D.pdf'), (0.23890807, '1500.47C.pdf'), (0.2384286, '1550.13.pdf'), (0.2371416, '1601.10D.pdf'), (0.23595658, '1120.12A.pdf'), (0.23468414, '1720.4B.pdf'), (0.23400822, '1700.10N.pdf'), (0.23261216, '1650.36.pdf'), (0.23238577, '1070.2C.pdf'), (0.2323694, '1223.1D.pdf'), (0.23139068, '1540.2F.pdf'), (0.23107854, '1710.10 w CH-1.pdf'), (0.22955261, '1650.37A.pdf'), (0.22909185, '1700.1164.pdf'), (0.22744207, '1120.9A OPNAV.pdf'), (0.22336063, '1500.22H.pdf'), (0.22175482, '1730.1E.pdf'), (0.22140494, '1700.1028.pdf'), (0.22040927, '1541.5.pdf'), (0.2183844, '1500.57C.pdf'), (0.21594328, '1300.18B.pdf'), (0.21219595, '1120.5B.pdf'), (0.20830974, '1601.7M.pdf'), (0.20433137, '1531.2C.pdf'), (0.20351186, '1650.24D.pdf'), (0.20191768, '1754.5C.pdf'), (0.20108008, '1330.2C.pdf'), (0.19663848, '1100.5.pdf'), (0.18798718, '1414.4D.pdf'), (0.18331318, '1754.4A.pdf'), (0.17880656, '1040.11D.pdf'), (0.17834665, 'C1510.9H.pdf'), (0.17684795, '1500.84.pdf'), (0.17647082, '1500.49E.pdf'), (0.1690267, '1650.25C.pdf'), (0.1689494, '1750.5A.pdf'), (0.16484681, '1000.24D.pdf'), (0.16266897, '1650.32.pdf'), (0.15982248, '1120.11A.pdf'), (0.15786982, '1120.7A.pdf'), (0.15665843, '1001.26C.pdf'), (0.14485031, '1620.2A.pdf'), (0.1437001, '1160.9A.pdf'), (0.1389124, '1000.26B.pdf'), (0.1342704, '1000.16L With CH-2.pdf'), (0.13372521, '1650.26E.pdf'), (0.11576393, '1500.76C.pdf'), (0.1139891, '1750.3A.pdf'), (0.112301014, '1650.31.pdf'), (0.11051355, '1300.20.pdf'), (0.108783945, '1001.19D.pdf'), (0.108710155, '1120.8A.pdf'), (0.10839573, '1160.6C.pdf'), (0.102659985, '1752.3.pdf'), (0.09223834, '1120.4C.pdf'), (0.08946187, '1300.15B.pdf'), (0.0838795, '1500.75D.pdf'), (0.08364205, '1650.8D.pdf'), (0.08175868, '1120.6.pdf'), (0.08172585, '1427.1B.pdf'), (0.07316424, '1640.9A.pdf'), (0.06958489, '1620.3.pdf'), (0.057455085, ['1710.4C.pdf', 'Press Quality(1).joboptions']), (0.056836195, '1300.19A.pdf'), (0.056758784, '1500.82.pdf'), (0.055622637, '1120.10A.pdf'), (0.053003885, '1752.1C.pdf'), (0.051977105, '1220.1E.pdf'), (0.04953392, '1740.5D.pdf'), (0.04718007, '1160.8B.pdf'), (0.04590244, '1427.2A.pdf'), (0.033625398, '1100.4C.pdf'), (0.017283306, '1811.3A.pdf'), (0.0112519935, '1900.4A.pdf'), (0.008749627, '1752.2B.pdf'), (0.0051950533, '1710.7A.pdf'), (-0.0065217875, '1640.8A.pdf'), (-0.013036147, '1820.1B.pdf'), (-0.034902856, '1650.28B.pdf'), (-0.17320819, '1001.27.pdf'), (-0.17890784, '1001.20C.pdf'), (-0.1869691, '1320.6.pdf')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdjxZrxHFLwN",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv4x7xuEFNkw",
        "colab_type": "text"
      },
      "source": [
        "This model could be very useful in production. There is a huge corpus of instructions that USN personnel have to sort through in order to get information - an easy way to query this corpus to find relevant instructions given a topic would save everyone time and energy. There is no objective metric with which to measure 'accuracy' but the searches have consistently returned sane and sensible results during operator testing."
      ]
    }
  ]
}